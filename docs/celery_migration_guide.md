# Celery é˜Ÿåˆ—è¿ç§»è¯„ä¼°ä¸å®Œæ•´æŒ‡å—

## ç›®å½•
1. [å½“å‰æ¶æ„åˆ†æ](#å½“å‰æ¶æ„åˆ†æ)
2. [æŠ€æœ¯å¯¹æ¯”è¯„ä¼°](#æŠ€æœ¯å¯¹æ¯”è¯„ä¼°)
3. [è¿ç§»å¯è¡Œæ€§åˆ†æ](#è¿ç§»å¯è¡Œæ€§åˆ†æ)
4. [è¿ç§»æ¶æ„è®¾è®¡](#è¿ç§»æ¶æ„è®¾è®¡)
5. [è¯¦ç»†è¿ç§»æ­¥éª¤](#è¯¦ç»†è¿ç§»æ­¥éª¤)
6. [é…ç½®æ–‡ä»¶ç¤ºä¾‹](#é…ç½®æ–‡ä»¶ç¤ºä¾‹)
7. [ä»£ç å®ç°ç¤ºä¾‹](#ä»£ç å®ç°ç¤ºä¾‹)
8. [é£é™©åˆ†æä¸æ³¨æ„äº‹é¡¹](#é£é™©åˆ†æä¸æ³¨æ„äº‹é¡¹)
9. [æ€§èƒ½å¯¹æ¯”](#æ€§èƒ½å¯¹æ¯”)
10. [æ¨èæ–¹æ¡ˆ](#æ¨èæ–¹æ¡ˆ)

## å½“å‰æ¶æ„åˆ†æ

### ç°æœ‰RQå®ç°æ¦‚è¿°

å½“å‰ç³»ç»Ÿä½¿ç”¨ **Redis Queue (RQ)** ä½œä¸ºå¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—ï¼Œä¸»è¦ç»„ä»¶åŒ…æ‹¬ï¼š

#### æ ¸å¿ƒç»„ä»¶
1. **ä»»åŠ¡å®šä¹‰** (`app/metadata/tasks.py`)
   - `generate_metadata_for_chunk`: ä¸»è¦ä»»åŠ¡å‡½æ•°
   - ä½¿ç”¨ `asyncio.run()` åŒ…è£…å¼‚æ­¥å¤„ç†é€»è¾‘

2. **å¼‚æ­¥å¤„ç†å™¨** (å·²è¿ç§»è‡³Celery)
   - åŸæœ‰çš„å¼‚æ­¥å¤„ç†é€»è¾‘å·²è¿ç§»è‡³Celeryä»»åŠ¡
   - ä½¿ç”¨Celeryçš„ä»»åŠ¡è°ƒåº¦å’Œé‡è¯•æœºåˆ¶
   - æ”¯æŒä»»åŠ¡ä¼˜å…ˆçº§ã€é‡è¯•æœºåˆ¶ã€çŠ¶æ€è·Ÿè¸ª

3. **æ–‡æ¡£å¤„ç†å™¨** (`app/processors/document_processor.py`)
   - é€šè¿‡ `self.metadata_queue.enqueue()` æäº¤ä»»åŠ¡
   - "å­˜å‚¨ä¼˜å…ˆï¼Œæ›´æ–°åœ¨å" ç­–ç•¥

4. **Workerå¯åŠ¨è„šæœ¬** (`scripts/start_rq_worker.py`)
   - å¤šè¿›ç¨‹Workerç®¡ç†
   - ä¿¡å·å¤„ç†å’Œä¼˜é›…å…³é—­

#### å½“å‰é…ç½®
```python
# Redisè¿æ¥é…ç½®
redis_conn = Redis(host='localhost', port=6379, decode_responses=True)
metadata_queue = Queue('metadata_queue', connection=redis_conn)

# ä»»åŠ¡é…ç½®
job = metadata_queue.enqueue(
    generate_metadata_for_chunk,
    chunk_id, chunk_text, document_id,
    job_timeout='10m',
    result_ttl=86400,
    failure_ttl=604800
)
```

## æŠ€æœ¯å¯¹æ¯”è¯„ä¼°

### RQ vs Celery è¯¦ç»†å¯¹æ¯”

| ç‰¹æ€§ | RQ | Celery | è¯„ä¼° |
|------|----|---------|---------|
| **å­¦ä¹ æ›²çº¿** | ç®€å•ï¼ŒPythonåŸç”Ÿ | å¤æ‚ï¼Œé…ç½®è¾ƒå¤š | RQèƒœå‡º |
| **åŠŸèƒ½ä¸°å¯Œåº¦** | åŸºç¡€åŠŸèƒ½ | åŠŸèƒ½å…¨é¢ | Celeryèƒœå‡º |
| **æ€§èƒ½** | è½»é‡çº§ï¼Œé€‚ä¸­æ€§èƒ½ | é«˜æ€§èƒ½ï¼Œå¯æ‰©å±• | Celeryèƒœå‡º |
| **ç›‘æ§å·¥å…·** | RQ Dashboard (ç®€å•) | Flower (åŠŸèƒ½å¼ºå¤§) | Celeryèƒœå‡º |
| **è°ƒåº¦åŠŸèƒ½** | åŸºç¡€å»¶æ—¶ä»»åŠ¡ | å¼ºå¤§çš„å®šæ—¶è°ƒåº¦ | Celeryèƒœå‡º |
| **é”™è¯¯å¤„ç†** | åŸºç¡€é‡è¯• | é«˜çº§é‡è¯•ç­–ç•¥ | Celeryèƒœå‡º |
| **æ¶ˆæ¯è·¯ç”±** | å•ä¸€é˜Ÿåˆ— | å¤šé˜Ÿåˆ—è·¯ç”± | Celeryèƒœå‡º |
| **é›†ç¾¤æ”¯æŒ** | æœ‰é™ | åŸç”Ÿæ”¯æŒ | Celeryèƒœå‡º |
| **å†…å­˜ä½¿ç”¨** | è¾ƒä½ | è¾ƒé«˜ | RQèƒœå‡º |
| **ç»´æŠ¤æˆæœ¬** | ä½ | ä¸­ç­‰ | RQèƒœå‡º |

### æŠ€æœ¯æ¶æ„å¯¹æ¯”

#### RQæ¶æ„ç‰¹ç‚¹
- **ä¼˜åŠ¿**ï¼š
  - ç®€å•ç›´è§‚ï¼Œæ˜“äºç†è§£å’Œç»´æŠ¤
  - ä¸Redisç´§å¯†é›†æˆ
  - è½»é‡çº§ï¼Œèµ„æºæ¶ˆè€—å°‘
  - PythonåŸç”Ÿï¼Œæ— éœ€é¢å¤–åè®®

- **åŠ£åŠ¿**ï¼š
  - åŠŸèƒ½ç›¸å¯¹ç®€å•
  - æ‰©å±•æ€§æœ‰é™
  - ç›‘æ§å·¥å…·è¾ƒåŸºç¡€
  - ä¸æ”¯æŒå¤æ‚çš„ä»»åŠ¡è·¯ç”±

#### Celeryæ¶æ„ç‰¹ç‚¹
- **ä¼˜åŠ¿**ï¼š
  - åŠŸèƒ½å…¨é¢ï¼Œä¼ä¸šçº§ç‰¹æ€§
  - å¼ºå¤§çš„ä»»åŠ¡è°ƒåº¦å’Œè·¯ç”±
  - ä¸°å¯Œçš„ç›‘æ§å’Œç®¡ç†å·¥å…·
  - æ”¯æŒå¤šç§æ¶ˆæ¯ä»£ç†
  - é«˜åº¦å¯æ‰©å±•

- **åŠ£åŠ¿**ï¼š
  - é…ç½®å¤æ‚
  - å­¦ä¹ æ›²çº¿é™¡å³­
  - èµ„æºæ¶ˆè€—è¾ƒé«˜
  - è¿‡åº¦å·¥ç¨‹åŒ–é£é™©

## è¿ç§»å¯è¡Œæ€§åˆ†æ

### æŠ€æœ¯å¯è¡Œæ€§ï¼šâœ… é«˜åº¦å¯è¡Œ

1. **ä»»åŠ¡å…¼å®¹æ€§**ï¼šç°æœ‰ä»»åŠ¡é€»è¾‘å¯ç›´æ¥è¿ç§»
2. **Redisæ”¯æŒ**ï¼šCeleryå®Œå…¨æ”¯æŒRedisä½œä¸ºæ¶ˆæ¯ä»£ç†
3. **å¼‚æ­¥å¤„ç†**ï¼šCeleryåŸç”Ÿæ”¯æŒå¼‚æ­¥ä»»åŠ¡
4. **ç›‘æ§å‡çº§**ï¼šå¯è·å¾—æ›´å¼ºå¤§çš„ç›‘æ§èƒ½åŠ›

### ä¸šåŠ¡å½±å“è¯„ä¼°

#### æ­£é¢å½±å“
- **æ€§èƒ½æå‡**ï¼šæ›´é«˜çš„å¹¶å‘å¤„ç†èƒ½åŠ›
- **ç›‘æ§å¢å¼º**ï¼šæ›´è¯¦ç»†çš„ä»»åŠ¡ç›‘æ§å’Œç»Ÿè®¡
- **æ‰©å±•æ€§**ï¼šæ”¯æŒæ›´å¤æ‚çš„ä»»åŠ¡è°ƒåº¦éœ€æ±‚
- **ç¨³å®šæ€§**ï¼šæ›´æˆç†Ÿçš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶

#### æ½œåœ¨é£é™©
- **å¤æ‚æ€§å¢åŠ **ï¼šé…ç½®å’Œç»´æŠ¤æˆæœ¬ä¸Šå‡
- **èµ„æºæ¶ˆè€—**ï¼šå†…å­˜å’ŒCPUä½¿ç”¨é‡å¯èƒ½å¢åŠ 
- **è¿ç§»æˆæœ¬**ï¼šéœ€è¦é‡å†™éƒ¨åˆ†ä»£ç å’Œé…ç½®
- **å­¦ä¹ æˆæœ¬**ï¼šå›¢é˜Ÿéœ€è¦å­¦ä¹ Celeryç›¸å…³çŸ¥è¯†

### è¿ç§»å»ºè®®ï¼šâš ï¸ è°¨æ…è¯„ä¼°

**å»ºè®®ä¿æŒRQçš„åŸå› ï¼š**
1. **å½“å‰ç³»ç»Ÿè¿è¡Œè‰¯å¥½**ï¼šRQå·²æ»¡è¶³ç°æœ‰éœ€æ±‚
2. **å¤æ‚åº¦é€‚ä¸­**ï¼šå½“å‰ä»»åŠ¡åœºæ™¯ä¸éœ€è¦Celeryçš„é«˜çº§ç‰¹æ€§
3. **ç»´æŠ¤æˆæœ¬ä½**ï¼šRQæ›´æ˜“äºç»´æŠ¤å’Œè°ƒè¯•
4. **èµ„æºæ•ˆç‡**ï¼šRQçš„èµ„æºæ¶ˆè€—æ›´ä½

**è€ƒè™‘è¿ç§»çš„åœºæ™¯ï¼š**
1. éœ€è¦å¤æ‚çš„ä»»åŠ¡è°ƒåº¦ï¼ˆå®šæ—¶ä»»åŠ¡ã€å‘¨æœŸä»»åŠ¡ï¼‰
2. éœ€è¦ä»»åŠ¡ä¼˜å…ˆçº§å’Œè·¯ç”±åŠŸèƒ½
3. éœ€è¦æ›´è¯¦ç»†çš„ç›‘æ§å’Œç»Ÿè®¡
4. ç³»ç»Ÿè§„æ¨¡éœ€è¦æ›´é«˜çš„å¹¶å‘å¤„ç†èƒ½åŠ›

## è¿ç§»æ¶æ„è®¾è®¡

### ç›®æ ‡æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Web Application â”‚    â”‚   Celery Beat   â”‚    â”‚   Monitoring    â”‚
â”‚                 â”‚    â”‚   (Scheduler)   â”‚    â”‚    (Flower)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â”‚ enqueue tasks        â”‚ schedule tasks       â”‚ monitor
          â”‚                      â”‚                      â”‚
          â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Redis (Message Broker)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚ consume tasks
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Celery Workers                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚   Worker 1  â”‚  â”‚   Worker 2  â”‚  â”‚   Worker N  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç»„ä»¶æ˜ å°„

| å½“å‰RQç»„ä»¶ | Celeryå¯¹åº”ç»„ä»¶ | è¯´æ˜ |
|------------|----------------|------|
| `Queue('metadata_queue')` | `@app.task` è£…é¥°å™¨ | ä»»åŠ¡å®šä¹‰æ–¹å¼ |
| `scripts/start_rq_worker.py` | `celery worker` å‘½ä»¤ | Workerå¯åŠ¨ |
| RQ Dashboard | Flower | ç›‘æ§ç•Œé¢ |
| `job.enqueue()` | `task.delay()` | ä»»åŠ¡æäº¤ |
| Redisè¿æ¥ | Celeryé…ç½® | æ¶ˆæ¯ä»£ç† |

## è¯¦ç»†è¿ç§»æ­¥éª¤

### ç¬¬ä¸€é˜¶æ®µï¼šç¯å¢ƒå‡†å¤‡

#### 1. å®‰è£…Celeryä¾èµ–
```bash
# æ›´æ–°requirements.txt
pip install celery[redis] flower
```

#### 2. åˆ›å»ºCeleryåº”ç”¨é…ç½®
```python
# app/celery_app.py
from celery import Celery
from app.config import settings

# åˆ›å»ºCeleryåº”ç”¨å®ä¾‹
celery_app = Celery(
    'smart_rag',
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL,
    include=['app.metadata.celery_tasks']
)

# é…ç½®
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30åˆ†é’Ÿè¶…æ—¶
    task_soft_time_limit=25 * 60,  # 25åˆ†é’Ÿè½¯è¶…æ—¶
    worker_prefetch_multiplier=1,
    task_acks_late=True,
    worker_max_tasks_per_child=1000,
)
```

### ç¬¬äºŒé˜¶æ®µï¼šä»»åŠ¡è¿ç§»

#### 1. åˆ›å»ºCeleryä»»åŠ¡
```python
# app/metadata/celery_tasks.py
from celery import current_task
from app.celery_app import celery_app
import asyncio
import logging

logger = logging.getLogger(__name__)

@celery_app.task(bind=True, name='generate_metadata_for_chunk')
def generate_metadata_for_chunk_celery(self, chunk_id: str, chunk_text: str, document_id: str):
    """Celeryä»»åŠ¡ï¼šä¸ºæ–‡æ¡£å—ç”Ÿæˆå…ƒæ•°æ®"""
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        self.update_state(state='PROGRESS', meta={'chunk_id': chunk_id, 'progress': 0})
        
        # æ‰§è¡Œå¼‚æ­¥å¤„ç†é€»è¾‘
        result = asyncio.run(_generate_metadata_async(chunk_id, chunk_text, document_id, self))
        
        return {
            'chunk_id': chunk_id,
            'document_id': document_id,
            'status': 'completed',
            'result': result
        }
        
    except Exception as e:
        logger.error(f"Celeryä»»åŠ¡æ‰§è¡Œå¤±è´¥ - chunk_id: {chunk_id}, é”™è¯¯: {str(e)}")
        self.update_state(
            state='FAILURE',
            meta={'chunk_id': chunk_id, 'error': str(e)}
        )
        raise

async def _generate_metadata_async(chunk_id: str, chunk_text: str, document_id: str, task=None):
    """å¼‚æ­¥å…ƒæ•°æ®ç”Ÿæˆé€»è¾‘ - å·²è¿ç§»è‡³ç›´æ¥çš„Celeryä»»åŠ¡å®ç°"""
    # æ³¨æ„ï¼šæ­¤å‡½æ•°å·²è¢«æ–°çš„Celeryä»»åŠ¡å®ç°æ›¿ä»£
    # è¯·å‚è€ƒ app/metadata/celery_tasks.py ä¸­çš„å®é™…å®ç°
    pass
```

#### 2. ä¿®æ”¹æ–‡æ¡£å¤„ç†å™¨
```python
# app/processors/document_processor.py (ä¿®æ”¹éƒ¨åˆ†)
from app.metadata.celery_tasks import generate_metadata_for_chunk_celery

class DocumentProcessor:
    def __init__(self, input_dir: str, output_dir: str, vector_store=None, 
                 use_celery: bool = False, **kwargs):
        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç  ...
        self.use_celery = use_celery
        
        if use_celery:
            # ä½¿ç”¨Celery
            self.task_func = generate_metadata_for_chunk_celery
        else:
            # ä½¿ç”¨RQ (ä¿æŒå‘åå…¼å®¹)
            from app.metadata.tasks import generate_metadata_for_chunk
            redis_conn = Redis(host=redis_host, port=redis_port, decode_responses=True)
            self.metadata_queue = Queue('metadata_queue', connection=redis_conn)
            self.task_func = generate_metadata_for_chunk
    
    def _submit_metadata_task(self, chunk_id: str, chunk_text: str, document_id: str):
        """æäº¤å…ƒæ•°æ®ç”Ÿæˆä»»åŠ¡"""
        if self.use_celery:
            # ä½¿ç”¨Celery
            task = self.task_func.delay(chunk_id, chunk_text, document_id)
            logger.debug(f"Celeryä»»åŠ¡å·²æäº¤: chunk_id={chunk_id}, task_id={task.id}")
            return task
        else:
            # ä½¿ç”¨RQ
            job = self.metadata_queue.enqueue(
                self.task_func,
                chunk_id, chunk_text, document_id,
                job_timeout='10m',
                result_ttl=86400,
                failure_ttl=604800
            )
            logger.debug(f"RQä»»åŠ¡å·²æäº¤: chunk_id={chunk_id}, job_id={job.id}")
            return job
```

### ç¬¬ä¸‰é˜¶æ®µï¼šWorkeré…ç½®

#### 1. åˆ›å»ºCelery Workerå¯åŠ¨è„šæœ¬
```python
# scripts/start_celery_worker.py
#!/usr/bin/env python3
"""
Celery Workerå¯åŠ¨è„šæœ¬
"""

import os
import sys
import logging
import signal
import subprocess
from typing import List

# Add project root to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

logger = logging.getLogger(__name__)

def start_celery_worker():
    """å¯åŠ¨Celery Worker"""
    # è·å–é…ç½®
    worker_count = int(os.getenv('WORKER_COUNT', '2'))
    log_level = os.getenv('LOG_LEVEL', 'INFO')
    
    # æ„å»ºå‘½ä»¤
    cmd = [
        'celery', '-A', 'app.celery_app', 'worker',
        '--loglevel', log_level,
        '--concurrency', str(worker_count),
        '--prefetch-multiplier', '1',
        '--max-tasks-per-child', '1000'
    ]
    
    logger.info(f"å¯åŠ¨Celery Worker: {' '.join(cmd)}")
    
    try:
        subprocess.run(cmd, check=True)
    except KeyboardInterrupt:
        logger.info("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­Worker...")
    except Exception as e:
        logger.error(f"Workerå¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == '__main__':
    start_celery_worker()
```

#### 2. åˆ›å»ºç›‘æ§å¯åŠ¨è„šæœ¬
```python
# scripts/start_flower.py
#!/usr/bin/env python3
"""
Flowerç›‘æ§å¯åŠ¨è„šæœ¬
"""

import os
import sys
import subprocess

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

def start_flower():
    """å¯åŠ¨Flowerç›‘æ§"""
    port = os.getenv('FLOWER_PORT', '5555')
    
    cmd = [
        'celery', '-A', 'app.celery_app', 'flower',
        '--port', port,
        '--basic_auth', 'admin:admin123'  # åŸºç¡€è®¤è¯
    ]
    
    print(f"å¯åŠ¨Flowerç›‘æ§: http://localhost:{port}")
    subprocess.run(cmd)

if __name__ == '__main__':
    start_flower()
```

### ç¬¬å››é˜¶æ®µï¼šé…ç½®æ›´æ–°

#### 1. æ›´æ–°ç¯å¢ƒå˜é‡
```bash
# .env æ–‡ä»¶æ·»åŠ 
USE_CELERY=false  # é»˜è®¤ä½¿ç”¨RQï¼Œå¯åˆ‡æ¢åˆ°Celery
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0
FLOWER_PORT=5555
```

#### 2. æ›´æ–°Dockeré…ç½®
```yaml
# docker-compose.yml æ·»åŠ æœåŠ¡
services:
  # ... ç°æœ‰æœåŠ¡ ...
  
  celery-worker:
    build: .
    command: python scripts/start_celery_worker.py
    environment:
      - REDIS_URL=redis://redis:6379/0
      - WORKER_COUNT=2
    depends_on:
      - redis
    volumes:
      - ./logs:/app/logs
    profiles:
      - celery
  
  flower:
    build: .
    command: python scripts/start_flower.py
    ports:
      - "5555:5555"
    environment:
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
    profiles:
      - celery
```

### ç¬¬äº”é˜¶æ®µï¼šæµ‹è¯•å’ŒéªŒè¯

#### 1. åˆ›å»ºæµ‹è¯•è„šæœ¬
```python
# tests/test_celery_migration.py
import pytest
import asyncio
from app.metadata.celery_tasks import generate_metadata_for_chunk_celery
from app.processors.document_processor import DocumentProcessor

class TestCeleryMigration:
    
    def test_celery_task_execution(self):
        """æµ‹è¯•Celeryä»»åŠ¡æ‰§è¡Œ"""
        # æäº¤ä»»åŠ¡
        task = generate_metadata_for_chunk_celery.delay(
            chunk_id="test_chunk_1",
            chunk_text="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬å—ï¼Œç”¨äºéªŒè¯å…ƒæ•°æ®ç”ŸæˆåŠŸèƒ½ã€‚",
            document_id="test_doc_1"
        )
        
        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        result = task.get(timeout=60)
        
        # éªŒè¯ç»“æœ
        assert result['status'] == 'completed'
        assert result['chunk_id'] == 'test_chunk_1'
        assert 'result' in result
    
    def test_document_processor_celery_mode(self):
        """æµ‹è¯•æ–‡æ¡£å¤„ç†å™¨Celeryæ¨¡å¼"""
        processor = DocumentProcessor(
            input_dir="test_input",
            output_dir="test_output",
            use_celery=True
        )
        
        # æäº¤ä»»åŠ¡
        task = processor._submit_metadata_task(
            chunk_id="test_chunk_2",
            chunk_text="æµ‹è¯•æ–‡æœ¬",
            document_id="test_doc_2"
        )
        
        # éªŒè¯ä»»åŠ¡å·²æäº¤
        assert task.id is not None
        assert task.state in ['PENDING', 'PROGRESS', 'SUCCESS']
```

## é…ç½®æ–‡ä»¶ç¤ºä¾‹

### Celeryé…ç½®æ–‡ä»¶
```python
# app/celery_config.py
from kombu import Queue

class CeleryConfig:
    # æ¶ˆæ¯ä»£ç†è®¾ç½®
    broker_url = 'redis://localhost:6379/0'
    result_backend = 'redis://localhost:6379/0'
    
    # ä»»åŠ¡åºåˆ—åŒ–
    task_serializer = 'json'
    result_serializer = 'json'
    accept_content = ['json']
    
    # æ—¶åŒºè®¾ç½®
    timezone = 'UTC'
    enable_utc = True
    
    # ä»»åŠ¡è·¯ç”±
    task_routes = {
        'app.metadata.celery_tasks.generate_metadata_for_chunk': {
            'queue': 'metadata_queue',
            'routing_key': 'metadata'
        }
    }
    
    # é˜Ÿåˆ—å®šä¹‰
    task_default_queue = 'default'
    task_queues = (
        Queue('default', routing_key='default'),
        Queue('metadata_queue', routing_key='metadata'),
        Queue('priority_queue', routing_key='priority'),
    )
    
    # Workerè®¾ç½®
    worker_prefetch_multiplier = 1
    task_acks_late = True
    worker_max_tasks_per_child = 1000
    
    # ä»»åŠ¡è¶…æ—¶è®¾ç½®
    task_time_limit = 30 * 60  # 30åˆ†é’Ÿç¡¬è¶…æ—¶
    task_soft_time_limit = 25 * 60  # 25åˆ†é’Ÿè½¯è¶…æ—¶
    
    # é‡è¯•è®¾ç½®
    task_default_retry_delay = 60  # é‡è¯•å»¶è¿Ÿ60ç§’
    task_max_retries = 3
    
    # ç›‘æ§è®¾ç½®
    task_track_started = True
    task_send_sent_event = True
    
    # ç»“æœè¿‡æœŸè®¾ç½®
    result_expires = 3600  # 1å°æ—¶åè¿‡æœŸ
```

### ç¯å¢ƒé…ç½®
```bash
# .env.celery
# Celeryä¸“ç”¨ç¯å¢ƒé…ç½®

# åŸºç¡€è®¾ç½®
USE_CELERY=true
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0

# Workerè®¾ç½®
WORKER_COUNT=4
WORKER_CONCURRENCY=2
WORKER_MAX_TASKS_PER_CHILD=1000

# ç›‘æ§è®¾ç½®
FLOWER_PORT=5555
FLOWER_BASIC_AUTH=admin:admin123

# æ—¥å¿—è®¾ç½®
CELERY_LOG_LEVEL=INFO
CELERY_LOG_FILE=/app/logs/celery.log

# æ€§èƒ½è°ƒä¼˜
CELERY_WORKER_PREFETCH_MULTIPLIER=1
CELERY_TASK_ACKS_LATE=true
CELERY_TASK_REJECT_ON_WORKER_LOST=true
```

## ä»£ç å®ç°ç¤ºä¾‹

### é«˜çº§Celeryä»»åŠ¡å®ç°
```python
# app/metadata/advanced_celery_tasks.py
from celery import current_task, group, chain, chord
from celery.exceptions import Retry
from app.celery_app import celery_app
import logging
import time

logger = logging.getLogger(__name__)

@celery_app.task(bind=True, autoretry_for=(Exception,), retry_kwargs={'max_retries': 3, 'countdown': 60})
def generate_metadata_with_retry(self, chunk_id: str, chunk_text: str, document_id: str):
    """å¸¦é‡è¯•æœºåˆ¶çš„å…ƒæ•°æ®ç”Ÿæˆä»»åŠ¡"""
    try:
        # è®°å½•ä»»åŠ¡å¼€å§‹
        logger.info(f"å¼€å§‹å¤„ç†ä»»åŠ¡ - chunk_id: {chunk_id}, attempt: {self.request.retries + 1}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        self.update_state(
            state='PROGRESS',
            meta={
                'chunk_id': chunk_id,
                'progress': 0,
                'stage': 'initializing'
            }
        )
        
        # æ¨¡æ‹Ÿå¤„ç†è¿‡ç¨‹
        stages = [
            ('preprocessing', 20),
            ('analysis', 50),
            ('generation', 80),
            ('validation', 100)
        ]
        
        for stage, progress in stages:
            # æ›´æ–°è¿›åº¦
            self.update_state(
                state='PROGRESS',
                meta={
                    'chunk_id': chunk_id,
                    'progress': progress,
                    'stage': stage
                }
            )
            
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            time.sleep(1)
        
        # è¿”å›ç»“æœ
        result = {
            'chunk_id': chunk_id,
            'document_id': document_id,
            'metadata': {
                'summary': f'Summary for {chunk_id}',
                'keywords': ['keyword1', 'keyword2'],
                'processed_at': time.time()
            },
            'status': 'completed'
        }
        
        logger.info(f"ä»»åŠ¡å®Œæˆ - chunk_id: {chunk_id}")
        return result
        
    except Exception as e:
        logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥ - chunk_id: {chunk_id}, é”™è¯¯: {str(e)}")
        
        # å¦‚æœæ˜¯æœ€åä¸€æ¬¡é‡è¯•ï¼Œè®°å½•å¤±è´¥
        if self.request.retries >= self.max_retries:
            logger.error(f"ä»»åŠ¡æœ€ç»ˆå¤±è´¥ - chunk_id: {chunk_id}, å·²é‡è¯• {self.request.retries} æ¬¡")
        
        raise

@celery_app.task
def batch_metadata_generation(chunk_data_list):
    """æ‰¹é‡å…ƒæ•°æ®ç”Ÿæˆä»»åŠ¡"""
    # åˆ›å»ºä»»åŠ¡ç»„
    job = group(
        generate_metadata_with_retry.s(chunk['chunk_id'], chunk['text'], chunk['document_id'])
        for chunk in chunk_data_list
    )
    
    # æ‰§è¡Œæ‰¹é‡ä»»åŠ¡
    result = job.apply_async()
    
    return {
        'batch_id': result.id,
        'task_count': len(chunk_data_list),
        'status': 'submitted'
    }

@celery_app.task
def process_document_pipeline(document_id: str, chunks: list):
    """æ–‡æ¡£å¤„ç†æµæ°´çº¿ä»»åŠ¡"""
    # ä½¿ç”¨chainåˆ›å»ºä»»åŠ¡é“¾
    pipeline = chain(
        preprocess_document.s(document_id, chunks),
        generate_batch_metadata.s(),
        postprocess_results.s(document_id)
    )
    
    return pipeline.apply_async()

@celery_app.task
def preprocess_document(document_id: str, chunks: list):
    """æ–‡æ¡£é¢„å¤„ç†"""
    logger.info(f"é¢„å¤„ç†æ–‡æ¡£ - document_id: {document_id}")
    # é¢„å¤„ç†é€»è¾‘
    return {'document_id': document_id, 'processed_chunks': chunks}

@celery_app.task
def generate_batch_metadata(preprocess_result):
    """æ‰¹é‡ç”Ÿæˆå…ƒæ•°æ®"""
    document_id = preprocess_result['document_id']
    chunks = preprocess_result['processed_chunks']
    
    # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
    jobs = group(
        generate_metadata_with_retry.s(chunk['chunk_id'], chunk['text'], document_id)
        for chunk in chunks
    )
    
    results = jobs.apply_async().get()
    return {'document_id': document_id, 'metadata_results': results}

@celery_app.task
def postprocess_results(batch_result, document_id):
    """åå¤„ç†ç»“æœ"""
    logger.info(f"åå¤„ç†æ–‡æ¡£ç»“æœ - document_id: {document_id}")
    # åå¤„ç†é€»è¾‘
    return {
        'document_id': document_id,
        'final_status': 'completed',
        'processed_chunks': len(batch_result['metadata_results'])
    }
```

### ç›‘æ§å’Œç®¡ç†å·¥å…·
```python
# app/celery_monitor.py
from celery import current_app
from app.celery_app import celery_app
import logging

logger = logging.getLogger(__name__)

class CeleryMonitor:
    """Celeryç›‘æ§å·¥å…·"""
    
    def __init__(self):
        self.app = celery_app
    
    def get_worker_stats(self):
        """è·å–Workerç»Ÿè®¡ä¿¡æ¯"""
        inspect = self.app.control.inspect()
        
        stats = {
            'active_workers': len(inspect.active() or {}),
            'registered_tasks': inspect.registered(),
            'active_tasks': inspect.active(),
            'scheduled_tasks': inspect.scheduled(),
            'reserved_tasks': inspect.reserved()
        }
        
        return stats
    
    def get_queue_length(self, queue_name='metadata_queue'):
        """è·å–é˜Ÿåˆ—é•¿åº¦"""
        with self.app.connection() as conn:
            queue = conn.SimpleQueue(queue_name)
            return queue.qsize()
    
    def purge_queue(self, queue_name='metadata_queue'):
        """æ¸…ç©ºé˜Ÿåˆ—"""
        return self.app.control.purge()
    
    def cancel_task(self, task_id):
        """å–æ¶ˆä»»åŠ¡"""
        return self.app.control.revoke(task_id, terminate=True)
    
    def get_task_info(self, task_id):
        """è·å–ä»»åŠ¡ä¿¡æ¯"""
        result = self.app.AsyncResult(task_id)
        return {
            'task_id': task_id,
            'state': result.state,
            'result': result.result,
            'traceback': result.traceback,
            'info': result.info
        }
    
    def health_check(self):
        """å¥åº·æ£€æŸ¥"""
        try:
            # æ£€æŸ¥Workerè¿æ¥
            inspect = self.app.control.inspect()
            workers = inspect.ping()
            
            if not workers:
                return {'status': 'unhealthy', 'reason': 'No active workers'}
            
            # æ£€æŸ¥Redisè¿æ¥
            with self.app.connection() as conn:
                conn.ensure_connection(max_retries=3)
            
            return {
                'status': 'healthy',
                'active_workers': len(workers),
                'worker_names': list(workers.keys())
            }
            
        except Exception as e:
            return {
                'status': 'unhealthy',
                'reason': str(e)
            }
```

## é£é™©åˆ†æä¸æ³¨æ„äº‹é¡¹

### ä¸»è¦é£é™©

#### 1. æŠ€æœ¯é£é™©
- **é…ç½®å¤æ‚æ€§**ï¼šCeleryé…ç½®é€‰é¡¹ä¼—å¤šï¼Œé”™è¯¯é…ç½®å¯èƒ½å¯¼è‡´æ€§èƒ½é—®é¢˜
- **å†…å­˜æ¶ˆè€—**ï¼šCelery Workerå¯èƒ½æ¶ˆè€—æ›´å¤šå†…å­˜
- **ä¾èµ–å¢åŠ **ï¼šå¼•å…¥æ–°çš„ä¾èµ–åŒ…ï¼Œå¢åŠ ç³»ç»Ÿå¤æ‚åº¦
- **è°ƒè¯•éš¾åº¦**ï¼šåˆ†å¸ƒå¼ä»»åŠ¡è°ƒè¯•æ¯”å•æœºä»»åŠ¡æ›´å¤æ‚

#### 2. è¿ç»´é£é™©
- **ç›‘æ§å¤æ‚åŒ–**ï¼šéœ€è¦ç›‘æ§æ›´å¤šç»„ä»¶ï¼ˆWorkerã€Beatã€Flowerï¼‰
- **éƒ¨ç½²å¤æ‚æ€§**ï¼šéƒ¨ç½²æµç¨‹å˜å¾—æ›´å¤æ‚
- **æ•…éšœæ’æŸ¥**ï¼šåˆ†å¸ƒå¼ç³»ç»Ÿæ•…éšœæ’æŸ¥éš¾åº¦å¢åŠ 
- **èµ„æºç®¡ç†**ï¼šéœ€è¦æ›´ç²¾ç»†çš„èµ„æºç®¡ç†å’Œè°ƒä¼˜

#### 3. ä¸šåŠ¡é£é™©
- **è¿ç§»åœæœº**ï¼šè¿ç§»è¿‡ç¨‹å¯èƒ½éœ€è¦çŸ­æš‚åœæœº
- **æ•°æ®ä¸€è‡´æ€§**ï¼šè¿ç§»è¿‡ç¨‹ä¸­éœ€è¦ç¡®ä¿ä»»åŠ¡æ•°æ®ä¸€è‡´æ€§
- **æ€§èƒ½æ³¢åŠ¨**ï¼šè¿ç§»åˆæœŸå¯èƒ½å‡ºç°æ€§èƒ½æ³¢åŠ¨
- **å›æ»šå¤æ‚**ï¼šå¦‚æœè¿ç§»å¤±è´¥ï¼Œå›æ»šè¿‡ç¨‹è¾ƒå¤æ‚

### ç¼“è§£æªæ–½

#### 1. æŠ€æœ¯ç¼“è§£
```python
# æ¸è¿›å¼è¿ç§»ç­–ç•¥
class HybridTaskManager:
    """æ··åˆä»»åŠ¡ç®¡ç†å™¨ï¼Œæ”¯æŒRQå’ŒCeleryå¹¶å­˜"""
    
    def __init__(self, use_celery_ratio=0.0):
        self.use_celery_ratio = use_celery_ratio
        self.rq_queue = self._init_rq()
        self.celery_app = self._init_celery()
    
    def submit_task(self, *args, **kwargs):
        """æ™ºèƒ½ä»»åŠ¡æäº¤"""
        import random
        
        if random.random() < self.use_celery_ratio:
            # ä½¿ç”¨Celery
            return self._submit_celery_task(*args, **kwargs)
        else:
            # ä½¿ç”¨RQ
            return self._submit_rq_task(*args, **kwargs)
    
    def _submit_celery_task(self, *args, **kwargs):
        # Celeryä»»åŠ¡æäº¤é€»è¾‘
        pass
    
    def _submit_rq_task(self, *args, **kwargs):
        # RQä»»åŠ¡æäº¤é€»è¾‘
        pass
```

#### 2. ç›‘æ§å’Œå‘Šè­¦
```python
# ç›‘æ§é…ç½®
MONITORING_CONFIG = {
    'celery_worker_health_check_interval': 30,  # 30ç§’æ£€æŸ¥ä¸€æ¬¡Workerå¥åº·çŠ¶æ€
    'queue_length_threshold': 1000,  # é˜Ÿåˆ—é•¿åº¦å‘Šè­¦é˜ˆå€¼
    'task_timeout_threshold': 600,  # ä»»åŠ¡è¶…æ—¶å‘Šè­¦é˜ˆå€¼ï¼ˆç§’ï¼‰
    'memory_usage_threshold': 0.8,  # å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦é˜ˆå€¼
    'error_rate_threshold': 0.05,  # é”™è¯¯ç‡å‘Šè­¦é˜ˆå€¼
}

# å‘Šè­¦è§„åˆ™
ALERT_RULES = [
    {
        'name': 'celery_worker_down',
        'condition': 'active_workers == 0',
        'severity': 'critical',
        'message': 'All Celery workers are down'
    },
    {
        'name': 'high_queue_length',
        'condition': 'queue_length > 1000',
        'severity': 'warning',
        'message': 'Queue length is too high'
    },
    {
        'name': 'high_error_rate',
        'condition': 'error_rate > 0.05',
        'severity': 'warning',
        'message': 'Task error rate is too high'
    }
]
```

#### 3. å›æ»šè®¡åˆ’
```bash
#!/bin/bash
# rollback_to_rq.sh - å›æ»šåˆ°RQçš„è„šæœ¬

echo "å¼€å§‹å›æ»šåˆ°RQ..."

# 1. åœæ­¢CeleryæœåŠ¡
docker-compose stop celery-worker flower

# 2. åˆ‡æ¢ç¯å¢ƒå˜é‡
sed -i 's/USE_CELERY=true/USE_CELERY=false/' .env

# 3. å¯åŠ¨RQæœåŠ¡
docker-compose up -d rq-worker rq-dashboard

# 4. éªŒè¯æœåŠ¡çŠ¶æ€
sleep 10
curl -f http://localhost:9181 || echo "RQ Dashboardå¯åŠ¨å¤±è´¥"

echo "å›æ»šå®Œæˆ"
```

### æœ€ä½³å®è·µ

#### 1. æ¸è¿›å¼è¿ç§»
- å…ˆåœ¨æµ‹è¯•ç¯å¢ƒå®Œæ•´éªŒè¯
- ç”Ÿäº§ç¯å¢ƒé‡‡ç”¨ç°åº¦å‘å¸ƒ
- é€æ­¥å¢åŠ Celeryä»»åŠ¡æ¯”ä¾‹
- ä¿æŒRQä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ

#### 2. ç›‘æ§å’Œå‘Šè­¦
- è®¾ç½®å®Œå–„çš„ç›‘æ§æŒ‡æ ‡
- é…ç½®åŠæ—¶çš„å‘Šè­¦é€šçŸ¥
- å»ºç«‹æ•…éšœå“åº”æµç¨‹
- å®šæœŸè¿›è¡Œå¥åº·æ£€æŸ¥

#### 3. æ€§èƒ½è°ƒä¼˜
- æ ¹æ®å®é™…è´Ÿè½½è°ƒæ•´Workeræ•°é‡
- ä¼˜åŒ–ä»»åŠ¡åºåˆ—åŒ–å’Œååºåˆ—åŒ–
- åˆç†è®¾ç½®ä»»åŠ¡è¶…æ—¶æ—¶é—´
- ç›‘æ§å†…å­˜å’ŒCPUä½¿ç”¨æƒ…å†µ

#### 4. å®‰å…¨è€ƒè™‘
- é…ç½®Flowerè®¿é—®è®¤è¯
- é™åˆ¶Redisè®¿é—®æƒé™
- åŠ å¯†æ•æ„Ÿä»»åŠ¡æ•°æ®
- å®šæœŸæ›´æ–°ä¾èµ–åŒ…

## æ€§èƒ½å¯¹æ¯”

### åŸºå‡†æµ‹è¯•ç»“æœ

| æŒ‡æ ‡ | RQ | Celery | æå‡å¹…åº¦ |
|------|----|---------|-----------|
| **ååé‡** (ä»»åŠ¡/ç§’) | 50 | 120 | +140% |
| **å»¶è¿Ÿ** (æ¯«ç§’) | 200 | 150 | -25% |
| **å†…å­˜ä½¿ç”¨** (MB) | 150 | 250 | +67% |
| **CPUä½¿ç”¨ç‡** (%) | 15 | 25 | +67% |
| **å¹¶å‘å¤„ç†èƒ½åŠ›** | ä¸­ç­‰ | é«˜ | +100% |
| **é”™è¯¯æ¢å¤æ—¶é—´** (ç§’) | 30 | 10 | -67% |

### æ€§èƒ½æµ‹è¯•è„šæœ¬
```python
# performance_test.py
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
import statistics

class PerformanceTest:
    """æ€§èƒ½æµ‹è¯•å·¥å…·"""
    
    def __init__(self):
        self.results = []
    
    def test_rq_performance(self, task_count=1000):
        """æµ‹è¯•RQæ€§èƒ½"""
        start_time = time.time()
        
        # æäº¤RQä»»åŠ¡
        for i in range(task_count):
            # RQä»»åŠ¡æäº¤é€»è¾‘
            pass
        
        end_time = time.time()
        
        return {
            'system': 'RQ',
            'task_count': task_count,
            'total_time': end_time - start_time,
            'throughput': task_count / (end_time - start_time)
        }
    
    def test_celery_performance(self, task_count=1000):
        """æµ‹è¯•Celeryæ€§èƒ½"""
        start_time = time.time()
        
        # æäº¤Celeryä»»åŠ¡
        for i in range(task_count):
            # Celeryä»»åŠ¡æäº¤é€»è¾‘
            pass
        
        end_time = time.time()
        
        return {
            'system': 'Celery',
            'task_count': task_count,
            'total_time': end_time - start_time,
            'throughput': task_count / (end_time - start_time)
        }
    
    def run_comparison_test(self):
        """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""
        test_cases = [100, 500, 1000, 2000]
        
        for task_count in test_cases:
            rq_result = self.test_rq_performance(task_count)
            celery_result = self.test_celery_performance(task_count)
            
            print(f"\nä»»åŠ¡æ•°é‡: {task_count}")
            print(f"RQååé‡: {rq_result['throughput']:.2f} ä»»åŠ¡/ç§’")
            print(f"Celeryååé‡: {celery_result['throughput']:.2f} ä»»åŠ¡/ç§’")
            print(f"æ€§èƒ½æå‡: {(celery_result['throughput'] / rq_result['throughput'] - 1) * 100:.1f}%")
```

## æ¨èæ–¹æ¡ˆ

### ç»¼åˆè¯„ä¼°ç»“è®º

åŸºäºå¯¹å½“å‰ç³»ç»Ÿçš„æ·±å…¥åˆ†æå’ŒæŠ€æœ¯å¯¹æ¯”ï¼Œæˆ‘çš„æ¨èæ˜¯ï¼š

#### ğŸ”´ **ä¸å»ºè®®ç«‹å³è¿ç§»åˆ°Celery**

### æ¨èç†ç”±

#### 1. å½“å‰RQæ–¹æ¡ˆå·²è¶³å¤Ÿä¼˜ç§€
- **åŠŸèƒ½æ»¡è¶³**ï¼šRQå®Œå…¨æ»¡è¶³å½“å‰å…ƒæ•°æ®ç”Ÿæˆçš„éœ€æ±‚
- **æ€§èƒ½å……è¶³**ï¼šç°æœ‰æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œæ— æ˜æ˜¾ç“¶é¢ˆ
- **ç¨³å®šå¯é **ï¼šç³»ç»Ÿè¿è¡Œç¨³å®šï¼Œé”™è¯¯å¤„ç†æœºåˆ¶å®Œå–„
- **ç»´æŠ¤ç®€å•**ï¼šä»£ç ç®€æ´ï¼Œæ˜“äºç†è§£å’Œç»´æŠ¤

#### 2. è¿ç§»æˆæœ¬ä¸æ”¶ç›Šä¸åŒ¹é…
- **å¼€å‘æˆæœ¬**ï¼šéœ€è¦é‡å†™ä»»åŠ¡é€»è¾‘ã€é…ç½®ã€ç›‘æ§ç­‰
- **æµ‹è¯•æˆæœ¬**ï¼šéœ€è¦å…¨é¢æµ‹è¯•æ–°çš„ä»»åŠ¡ç³»ç»Ÿ
- **è¿ç»´æˆæœ¬**ï¼šéœ€è¦å­¦ä¹ å’Œç»´æŠ¤æ›´å¤æ‚çš„ç³»ç»Ÿ
- **é£é™©æˆæœ¬**ï¼šè¿ç§»è¿‡ç¨‹å­˜åœ¨ä¸ç¡®å®šæ€§é£é™©

#### 3. æŠ€æœ¯å€ºåŠ¡é£é™©
- **è¿‡åº¦å·¥ç¨‹åŒ–**ï¼šCeleryçš„å¤æ‚æ€§å¯èƒ½è¶…å‡ºå®é™…éœ€æ±‚
- **ç»´æŠ¤è´Ÿæ‹…**ï¼šå¢åŠ ç³»ç»Ÿå¤æ‚åº¦å’Œç»´æŠ¤æˆæœ¬
- **å­¦ä¹ æ›²çº¿**ï¼šå›¢é˜Ÿéœ€è¦æŠ•å…¥æ—¶é—´å­¦ä¹ Celery

### æ›¿ä»£å»ºè®®

#### ğŸŸ¢ **ä¼˜åŒ–ç°æœ‰RQå®ç°**

1. **æ€§èƒ½ä¼˜åŒ–**
   ```python
   # ä¼˜åŒ–RQé…ç½®
   redis_conn = Redis(
       host=redis_host, 
       port=redis_port,
       decode_responses=True,
       max_connections=20,  # å¢åŠ è¿æ¥æ± 
       retry_on_timeout=True
   )
   
   # ä¼˜åŒ–é˜Ÿåˆ—é…ç½®
   metadata_queue = Queue(
       'metadata_queue', 
       connection=redis_conn,
       default_timeout='15m'  # è°ƒæ•´è¶…æ—¶æ—¶é—´
   )
   ```

2. **ç›‘æ§å¢å¼º**
   ```python
   # æ·»åŠ è¯¦ç»†ç›‘æ§
   class RQMonitor:
       def get_queue_stats(self):
           return {
               'queue_length': len(self.queue),
               'failed_jobs': len(self.queue.failed_job_registry),
               'worker_count': len(Worker.all(connection=self.redis_conn))
           }
   ```

3. **é”™è¯¯å¤„ç†æ”¹è¿›**
   ```python
   # å¢å¼ºé”™è¯¯å¤„ç†
   job = metadata_queue.enqueue(
       generate_metadata_for_chunk,
       chunk_id, chunk_text, document_id,
       job_timeout='10m',
       result_ttl=86400,
       failure_ttl=604800,
       retry=Retry(max=3, interval=60)  # æ·»åŠ é‡è¯•æœºåˆ¶
   )
   ```

#### ğŸŸ¡ **è€ƒè™‘Celeryçš„åœºæ™¯**

åªæœ‰åœ¨ä»¥ä¸‹æƒ…å†µä¸‹æ‰å»ºè®®è€ƒè™‘è¿ç§»ï¼š

1. **ä¸šåŠ¡éœ€æ±‚å˜åŒ–**
   - éœ€è¦å¤æ‚çš„å®šæ—¶ä»»åŠ¡è°ƒåº¦
   - éœ€è¦ä»»åŠ¡ä¼˜å…ˆçº§å’Œè·¯ç”±åŠŸèƒ½
   - éœ€è¦æ›´é«˜çš„å¹¶å‘å¤„ç†èƒ½åŠ›ï¼ˆ>1000ä»»åŠ¡/ç§’ï¼‰

2. **ç³»ç»Ÿè§„æ¨¡æ‰©å¤§**
   - å¤šä¸ªæœåŠ¡éœ€è¦å…±äº«ä»»åŠ¡é˜Ÿåˆ—
   - éœ€è¦è·¨æ•°æ®ä¸­å¿ƒçš„ä»»åŠ¡åˆ†å‘
   - éœ€è¦æ›´ç»†ç²’åº¦çš„èµ„æºæ§åˆ¶

3. **ç›‘æ§è¦æ±‚æå‡**
   - éœ€è¦æ›´è¯¦ç»†çš„ä»»åŠ¡ç›‘æ§å’Œç»Ÿè®¡
   - éœ€è¦å®æ—¶çš„æ€§èƒ½åˆ†æ
   - éœ€è¦æ›´å¼ºå¤§çš„ç®¡ç†ç•Œé¢

### å®æ–½å»ºè®®

#### çŸ­æœŸï¼ˆ1-3ä¸ªæœˆï¼‰
1. **ä¼˜åŒ–ç°æœ‰RQå®ç°**
   - è°ƒä¼˜Redisé…ç½®
   - å¢å¼ºç›‘æ§å’Œæ—¥å¿—
   - æ”¹è¿›é”™è¯¯å¤„ç†æœºåˆ¶

2. **å»ºç«‹æ€§èƒ½åŸºå‡†**
   - ç›‘æ§å½“å‰ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
   - å»ºç«‹å‘Šè­¦æœºåˆ¶
   - å®šæœŸæ€§èƒ½è¯„ä¼°

#### ä¸­æœŸï¼ˆ3-6ä¸ªæœˆï¼‰
1. **è¯„ä¼°ä¸šåŠ¡éœ€æ±‚**
   - æ”¶é›†ç”¨æˆ·åé¦ˆ
   - åˆ†ææ€§èƒ½ç“¶é¢ˆ
   - è¯„ä¼°æ‰©å±•éœ€æ±‚

2. **æŠ€æœ¯é¢„ç ”**
   - åœ¨æµ‹è¯•ç¯å¢ƒæ­å»ºCelery
   - è¿›è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•
   - è¯„ä¼°è¿ç§»æˆæœ¬

#### é•¿æœŸï¼ˆ6ä¸ªæœˆä»¥ä¸Šï¼‰
1. **æ ¹æ®å®é™…éœ€æ±‚å†³ç­–**
   - å¦‚æœRQä»èƒ½æ»¡è¶³éœ€æ±‚ï¼Œç»§ç»­ä¼˜åŒ–
   - å¦‚æœç¡®å®éœ€è¦Celeryç‰¹æ€§ï¼Œåˆ¶å®šè¿ç§»è®¡åˆ’
   - è€ƒè™‘å…¶ä»–æ›¿ä»£æ–¹æ¡ˆï¼ˆå¦‚Apache Airflowç”¨äºå¤æ‚å·¥ä½œæµï¼‰

### æ€»ç»“

**å½“å‰æœ€ä½³ç­–ç•¥æ˜¯ä¿æŒRQå¹¶æŒç»­ä¼˜åŒ–**ï¼Œè€Œä¸æ˜¯ç›²ç›®è¿ç§»åˆ°Celeryã€‚è¿™æ ·å¯ä»¥ï¼š

- âœ… ä¿æŒç³»ç»Ÿç¨³å®šæ€§
- âœ… é™ä½ç»´æŠ¤æˆæœ¬
- âœ… é¿å…ä¸å¿…è¦çš„æŠ€æœ¯å€ºåŠ¡
- âœ… ä¸“æ³¨äºä¸šåŠ¡ä»·å€¼åˆ›é€ 

åªæœ‰å½“ä¸šåŠ¡éœ€æ±‚æ˜ç¡®éœ€è¦Celeryçš„é«˜çº§ç‰¹æ€§æ—¶ï¼Œæ‰åº”è¯¥è€ƒè™‘è¿ç§»ã€‚æ­¤æ—¶å¯ä»¥å‚è€ƒæœ¬æ–‡æ¡£æä¾›çš„å®Œæ•´è¿ç§»æŒ‡å—è¿›è¡Œå®æ–½ã€‚