# RAG组件详解之二：查询转换与优化

## 1. 概述

“Garbage in, garbage out.” 这句名言在RAG系统中同样适用。用户输入的原始查询（Query）往往是口语化的、模糊的，甚至可能包含错别字。如果直接将这样的查询用于检索，效果往往不佳。查询转换（Query Transformation）是RAG链条中承上启下的关键一环，它通过运用大型语言模型（LLM）自身的智能，对原始查询进行分析、重构和扩展，旨在生成一组更适合检索、更能“榨干”知识库信息的“超级查询”。

本系统的核心查询转换组件是`QueryTransformer`。

## 2. `QueryTransformer`：从用户语言到机器语言的桥梁

`QueryTransformer`的主要职责是解决用户查询与知识库文档之间的“语言鸿沟”。它包含两个核心功能：查询扩展和查询重写。

### 2.1. 查询扩展（Query Expansion）

**目标**：提高检索的**召回率（Recall）**。即，尽可能多地找到所有可能相关的文档，宁可错杀一千，不可放过一个。

**实现细节**：

1.  **利用LLM生成变体**：当接收到用户查询后，`QueryTransformer`会调用一个LLM（例如，一个较小、较快的模型，或者是像GPT-3.5这样的高效模型）。
2.  **精心设计的Prompt**：它会使用一个类似下面这样的Prompt来指令LLM：

    ```
    你是一个专业的检索助手。请根据以下用户问题，生成3个不同的、但语义上等价的查询。这些查询应该从不同的角度来提问，以便在向量数据库中进行更全面的搜索。

    用户问题："高血压的常见治疗方法有哪些？"
    ```

3.  **生成多个子查询**：LLM会返回一组查询，例如：
    - `“如何有效治疗高血压？”`
    - `“高血压的常用药物和非药物疗法是什么？”`
    - `“针对高血压患者，有哪些推荐的生活方式改变和医疗干预措施？”`

4.  **并行检索**：后续的`AdvancedFusionRetriever`（高级融合检索器）会接收这些子查询，并可以并行地在向量数据库中进行搜索。这意味着一次用户查询，实际上触发了多次后台检索，大大增加了找到相关信息的概率。

**优势**：
- **克服表述差异**：用户可能不知道知识库中使用的专业术语。查询扩展可以生成包含这些术语的变体。
- **挖掘隐含意图**：LLM能够理解用户的深层意图，并生成能够反映这些意图的查询。

### 2.2. 查询重写（Query Rewriting）

**目标**：提高检索的**精确率（Precision）**。即，让检索器返回的结果中，真正相关的文档比例更高。

**实现细节**：

1.  **优化查询结构**：与查询扩展生成多个新查询不同，查询重写旨在优化单个查询的质量。它会去除口语化表达、补充上下文、修正语法错误，使其更像一个“标准查询”。
2.  **上下文感知（Context-aware）**：在多轮对话中，查询重写尤为重要。例如：
    - **用户第一问**：`“什么是高血压？”`
    - **用户第二问**：`“那该怎么治？”`

    如果直接用`“那该怎么治？”`去检索，结果必然是混乱的。`QueryTransformer`在接收到第二问时，会结合上一轮的对话历史，将其重写为：`“高血压的治疗方法有哪些？”`。这个包含了完整上下文的新查询，才是有效的检索输入。

3.  **LLM的逻辑推理**：这个过程同样由LLM完成，它被要求理解对话的上下文，并生成一个独立的、完整的查询语句。

### 2.3. 代码示例（伪代码）

```python
class QueryTransformer:
    def __init__(self, llm_client):
        self.llm_client = llm_client

    def expand_query(self, query):
        prompt = f"请为以下问题生成3个不同的查询变体：{query}"
        # response是一个包含多个查询的列表
        expanded_queries = self.llm_client.generate(prompt)
        return expanded_queries

    def rewrite_query(self, query, chat_history):
        if not chat_history:
            return query

        context = "\n".join([f"Q: {turn['q']}\nA: {turn['a']}" for turn in chat_history])
        prompt = f"根据以下对话历史，将最后一个问题改写成一个独立的、完整的查询。\n\n历史：\n{context}\n\n最后一个问题：{query}"
        rewritten_query = self.llm_client.generate(prompt)
        return rewritten_query
```

## 3. 总结

`QueryTransformer`是RAG系统中一个“小而美”但至关重要的组件。它通过智能地利用LLM，在检索开始之前就对输入进行了优化，相当于为后续的检索、重排等重量级操作提供了更高质量的“弹药”。这一步的优化，以较小的计算成本，显著提升了整个RAG链条的效率和效果，是实现高性能RAG系统的关键实践之一。
